{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIMMX.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "Si1g1tjnEOMJ",
        "outputId": "688a9948-de66-4da5-806b-a528f6eb2b55"
      },
      "source": [
        "#after uploading the files from data folder comment this cell and execute all cells Runtime>Run all\r\n",
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-18a9bc4e-34ad-4a1f-9347-f158f220bd4d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-18a9bc4e-34ad-4a1f-9347-f158f220bd4d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving nlp-domain-pipeline.joblib to nlp-domain-pipeline.joblib\n",
            "Saving nlp-le-preprocessing.joblib to nlp-le-preprocessing.joblib\n",
            "Saving nlp-task-pipeline.joblib to nlp-task-pipeline.joblib\n",
            "Saving other-domain-pipeline.joblib to other-domain-pipeline.joblib\n",
            "Saving other-le-preprocessing.joblib to other-le-preprocessing.joblib\n",
            "Saving other-task-pipeline.joblib to other-task-pipeline.joblib\n",
            "Saving vision-domain-pipeline.joblib to vision-domain-pipeline.joblib\n",
            "Saving vision-le-preprocessing.joblib to vision-le-preprocessing.joblib\n",
            "Saving vision-task-pipeline.joblib to vision-task-pipeline.joblib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abj3Y4hrVyE5",
        "outputId": "f26430bc-de58-4cf0-90dd-d1ed290b2fde"
      },
      "source": [
        "pip install github3.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting github3.py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/21/9055a739fbe7b22a8e99e42906f2c75ba02bab9fd193a85837cd1d6e55d3/github3.py-1.3.0-py2.py3-none-any.whl (153kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 133kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 143kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from github3.py) (3.0.1)\n",
            "Collecting jwcrypto>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/a8/2ab318a407025203579bb4f45ec4ab80bff2c5a69c41ab39558bdf3c985d/jwcrypto-0.8-py2.py3-none-any.whl (79kB)\n",
            "\r\u001b[K     |████▏                           | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 20kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 30kB 27.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 40kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 51kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 71kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from github3.py) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from github3.py) (2.8.1)\n",
            "Collecting cryptography>=2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->github3.py) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->github3.py) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->github3.py) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->github3.py) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.0->github3.py) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3->jwcrypto>=0.5.0->github3.py) (1.14.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.3->jwcrypto>=0.5.0->github3.py) (2.20)\n",
            "Installing collected packages: cryptography, jwcrypto, github3.py\n",
            "Successfully installed cryptography-3.3.1 github3.py-1.3.0 jwcrypto-0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk1dtuKpM4Wj",
        "outputId": "05a45a72-88a6-42a4-b700-4a7b96ce4e03"
      },
      "source": [
        "pip install gitpython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 7.6MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.5 gitpython-3.1.12 smmap-3.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRm7Dk4N1nt7",
        "outputId": "d9f2f47c-4818-456c-f406-6a56cbf5a96e"
      },
      "source": [
        "pip install bibtexparser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bibtexparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/c3/c184a4460ba2f4877e3389e2d63479f642d0d3bdffeeffee0723d3b0156d/bibtexparser-1.2.0.tar.gz (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 30kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from bibtexparser) (2.4.7)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from bibtexparser) (0.16.0)\n",
            "Building wheels for collected packages: bibtexparser\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.2.0-cp36-none-any.whl size=36711 sha256=f2f60f268e4d5b9bc9c8f9ad5776021fb787b18a2184cd32ef23b5c9c17ae365\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5a/e7/867bcbc3a81c15b675b931aa19b6698375c5a5e90419a366db\n",
            "Successfully built bibtexparser\n",
            "Installing collected packages: bibtexparser\n",
            "Successfully installed bibtexparser-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuLLpTEy2yvf",
        "outputId": "e422c3eb-3677-42d5-cd97-7140dd511ab7"
      },
      "source": [
        "pip install arxiv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arxiv\n",
            "  Downloading https://files.pythonhosted.org/packages/50/81/9714d5a4efc14edddb308c0b527fe2d9ac35840fcfa83684a52655d35d42/arxiv-0.5.3-py3-none-any.whl\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from arxiv) (2.23.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->arxiv) (3.0.4)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6066 sha256=dbefbd1bc439639def4fde591cc7984d13a7651c3b61eb7043bbe043b565cc32\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-0.5.3 feedparser-6.0.2 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXpF11UgL6md",
        "outputId": "86cd3a74-c148-42e3-a27f-71bc91910713"
      },
      "source": [
        "pip install Repo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Repo\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/c8/17551b376aa62136cac5c0bc8205328f933f9efa1dc4a49e067e630d20a5/repo-0.1.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Repo) (2.23.0)\n",
            "Collecting configobj\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Repo) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Repo) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Repo) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Repo) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from configobj->Repo) (1.15.0)\n",
            "Building wheels for collected packages: Repo, configobj\n",
            "  Building wheel for Repo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Repo: filename=repo-0.1.0-cp36-none-any.whl size=12573 sha256=e66b9bd68dbf7001e0f3e456c611d5248f2369b2032b863c8eac89d278d61a97\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/c6/66/173760638daf5d57b54c7ee130caf874d26055cc16eb832012\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34545 sha256=34b986004bdb0a54f01ad24cd9aa696c057ffd27dd413a924a2995a95b9858fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built Repo configobj\n",
            "Installing collected packages: configobj, Repo\n",
            "Successfully installed Repo-0.1.0 configobj-5.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7SSpM3vVWIB"
      },
      "source": [
        "import sys\n",
        "import traceback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmeGLi9EVhdq"
      },
      "source": [
        "import os\n",
        "import fnmatch\n",
        "import tempfile\n",
        "import re\n",
        "import json\n",
        "from git import Repo\n",
        "\n",
        "def extract_modules(text):\n",
        "    importsRegEx = r\"(?:import|from) ([\\w\\-]+)((?:(,\\s*))[\\w\\-]+)*\\s*(?:\\s+as\\s+\\w*)?(?:import\\s\\w*)?[\\n.]\"\n",
        "    # importsRegEx = r\"(?:from|import)\\s+(\\w+)(?:\\s+as\\s+\\w*)?[\\n.]\"\n",
        "    mods = re.findall(importsRegEx, text)\n",
        "    # return mods\n",
        "    # if len(mods) > 0:\n",
        "    #     print(text, \" --- \", mods)\n",
        "    modules = []\n",
        "    for imp in mods:\n",
        "        for i in imp:\n",
        "            if len(i.strip()) > 0:\n",
        "                modules.append(i.replace(\",\",\"\").strip())\n",
        "    # # print(modules)\n",
        "    return modules\n",
        "\n",
        "def get_py_modules(org, repo, repoPath=\"\"):\n",
        "    # TODO implement directory\n",
        "    # TODO implement single file\n",
        "    # create a temporary directory\n",
        "    with tempfile.TemporaryDirectory() as directory:\n",
        "        # if org == \"tensorflow\":\n",
        "        #     return [\"tensorflow\"]\n",
        "        imports = []\n",
        "\n",
        "        Repo.clone_from(\"git://\"+\"github.com/\"+org+\"/\"+repo, directory, depth=1, no_checkout=False, single_branch=True)\n",
        "\n",
        "        if len(repoPath) > 0 and repoPath[0] != \"/\":\n",
        "            repoPath = \"/\"+repoPath\n",
        "        fileName = None\n",
        "        pathParts = repoPath.split(\"/\")\n",
        "        if pathParts[-1].find(\".\") > 0:\n",
        "            fileName = pathParts[-1]\n",
        "            repoPath = \"/\".join(pathParts[:-1])\n",
        "        # print(\"pathParts\", pathParts)\n",
        "        # print(\"repoPath\", repoPath)\n",
        "        # print(\"fileName\", fileName)\n",
        "        all_code_lines = []\n",
        "        for path, subdirs, files in os.walk(directory+repoPath):\n",
        "            innerPath = path[len(directory):]\n",
        "            if innerPath.startswith(\"/.\"):\n",
        "                continue\n",
        "            if \"init_net.pb\" in files and \"predict_net.pb\" in files:\n",
        "                if \"caffe2\" not in imports:\n",
        "                    imports.append(\"caffe2\")\n",
        "            for name in files:\n",
        "                if fileName is not None and name != fileName:\n",
        "                    continue\n",
        "                # print(path, innerPath, name)\n",
        "                if fnmatch.fnmatch(name, \"*.py\"):\n",
        "                    try:\n",
        "                        with open(os.path.join(path, name), mode=\"r\") as f:\n",
        "                            lines = f.read()\n",
        "                            all_code_lines += lines\n",
        "                            result = extract_modules(lines)\n",
        "                            for imp in result:\n",
        "                                if imp not in imports:\n",
        "                                    imports.append(imp)\n",
        "                    except Exception as e:\n",
        "                        print(\"Unable to read python file\", os.path.join(path, name))\n",
        "                if fnmatch.fnmatch(name, \"*.ipynb\"):\n",
        "                    try:\n",
        "                        with open(os.path.join(path, name), mode=\"r\") as f:\n",
        "                            code = json.load(f)\n",
        "                            if 'cells' in code:\n",
        "                                lines = []\n",
        "                                for cell in code['cells']:\n",
        "                                    if cell['cell_type'] == 'code':\n",
        "                                        for line in cell['source']:\n",
        "                                            lines.append(line)\n",
        "                                all_code_lines += lines\n",
        "                                result = extract_modules(\" \".join(lines))\n",
        "                                for imp in result:\n",
        "                                    if imp not in imports:\n",
        "                                        imports.append(imp)\n",
        "                    except Exception as e:\n",
        "                        print(\"Unable to parse ipynb file\", os.path.join(path, name))\n",
        "        #print(\"\".join(all_code_lines[:10]))\n",
        "        return sorted(imports)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nE5NpfSWT70"
      },
      "source": [
        "import json\n",
        "import base64, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ueajkbDW8r4"
      },
      "source": [
        "FRAMEWORKS = [\"tensorflow\", \"scikit-learn\", \"scikit_learn\", \"sklearn\", \"keras\", \"theanos\", \"torch\", \"caffe\", \"caffe2\", \"nltk\", \"theano\", \"lasagne\", \"mxnet\"]\n",
        "\n",
        "def repo_framework(repo_object):\n",
        "\n",
        "    frameworks = {}\n",
        "    requirements = get_file_from_repo(repo_object, \"requirements.txt\")\n",
        "    if requirements is None:\n",
        "        requirements = get_file_from_repo(repo_object, \"requirement.txt\")\n",
        "    if requirements is not None:\n",
        "        modules = requirements.split(\"\\n\")\n",
        "        for m in modules:\n",
        "            s = re.split('==|>=|~=|>',m.lower())\n",
        "            if s[0].strip() in FRAMEWORKS:\n",
        "                fw = s[0].strip()\n",
        "                frameworks[fw] = \"\"\n",
        "                if len(s) > 1:\n",
        "                    frameworks[fw] = s[1].strip()\n",
        "\n",
        "    return frameworks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuTwVQpvW-zo"
      },
      "source": [
        "import os\n",
        "# import json\n",
        "\n",
        "try:\n",
        "    import importlib.resources as pkg_resources\n",
        "except ImportError:\n",
        "    # Try backported to PY<37 `importlib_resources`.\n",
        "    import importlib_resources as pkg_resources\n",
        "\n",
        "libraries = {}\n",
        "\n",
        "fwArray = []\n",
        "fwEnv = os.getenv(\"frameworks\", None)\n",
        "if fwEnv is not None:\n",
        "    fwArray = fwEnv.split(\",\")\n",
        "else:\n",
        "    with open(\"frameworks.txt\", \"r\") as fwFile:\n",
        "        fwArray = fwFile.read().splitlines()\n",
        "\n",
        "# print(\"Frameworks array:\", \",\".join(fwArray))\n",
        "for f in fwArray:\n",
        "    if \":\" in f:\n",
        "        temp = f.split(\":\")\n",
        "        libraries[temp[0].strip().lower()] = temp[1].strip()\n",
        "    else:\n",
        "        libraries[f.strip().lower()] = f.strip()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeZaH3xXbUP9",
        "outputId": "c94965c6-40b1-4143-af5c-cf8209ab0b9a"
      },
      "source": [
        "\"\"\"\n",
        "    Utility functions for detecting datasets in a README file or similar\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "try:\n",
        "    import importlib.resources as pkg_resources\n",
        "except ImportError:\n",
        "    # Try backported to PY<37 `importlib_resources`.\n",
        "    import importlib_resources as pkg_resources\n",
        "\n",
        "LINK_PATTERN = \"\\[([^\\]]+?)\\]\\(([^\\)]+?)\\)\"\n",
        "ANCHOR_PATTERN = \"^#\\S+\"\n",
        "BOUNDRY_PATTERN = \"(\\s|\\W)?\"\n",
        "\n",
        "DATASET_LIST_PATH = \"sota-datasets.csv\"\n",
        "\n",
        "def detect_datasets_list(readme):\n",
        "    dataset_list = load_dataset_list()\n",
        "    list_datasets = find_dataset_from_list(readme, dataset_list)\n",
        "    return list_datasets\n",
        "\n",
        "def detect_datasets(readme, blob_link):\n",
        "    dataset_list = load_dataset_list()\n",
        "    datasets_found = []\n",
        "    link_datasets = get_dataset_links(readme, blob_link)\n",
        "    list_datasets = find_dataset_from_list(readme, dataset_list)\n",
        "    datasets_found += link_datasets\n",
        "\n",
        "    # resolve possible duplicates between the two, prioritize link datasets\n",
        "    for list_dataset in list_datasets:\n",
        "        skip = False\n",
        "        for link_dataset in link_datasets:\n",
        "            if list_dataset[\"name\"].lower() in link_dataset[\"name\"].lower():\n",
        "                skip = True\n",
        "                break\n",
        "        if not skip:\n",
        "            datasets_found.append(list_dataset)\n",
        "\n",
        "    #print(datasets_found)\n",
        "    return datasets_found\n",
        "\n",
        "def load_dataset_list(dataset_list_path=DATASET_LIST_PATH):\n",
        "    datasets = set()\n",
        "    header = True\n",
        "    with open(dataset_list_path,\"r\") as f:\n",
        "        for line in f:\n",
        "            datasets.add(line.strip())\n",
        "    return datasets\n",
        "\n",
        "def find_dataset_from_list(readme, dataset_list):\n",
        "    datasets_found = []\n",
        "    for dataset in dataset_list:\n",
        "        boundry_re = re.compile(\"(\\W|\\A)\" + dataset.lower() + \"(\\W|\\Z)\", re.M)\n",
        "        #print(boundry_re)\n",
        "        #print(boundry_re.search(readme.lower()))\n",
        "        if boundry_re.search(readme.lower()):\n",
        "            d = {\n",
        "                \"name\": dataset\n",
        "            }\n",
        "            datasets_found.append(d)\n",
        "    return datasets_found\n",
        "\n",
        "def get_dataset_links(readme, blob_link):\n",
        "    link_re = re.compile(LINK_PATTERN)\n",
        "\n",
        "    # extract out links and look for \"dataset or data set\" and an actual link\n",
        "    matches = link_re.findall(readme)\n",
        "    datasets = []\n",
        "    if (matches and len(matches) > 0):\n",
        "        for m in matches:\n",
        "            if \"dataset\" in m[0].lower() or \"data set\" in m[0].lower() or \"corpus\" in m[0].lower():\n",
        "                # ignore if anchor\n",
        "                anchor_re = re.compile(ANCHOR_PATTERN)\n",
        "                anchors = anchor_re.search(m[1])\n",
        "                dataset = {}\n",
        "                if anchors:\n",
        "                    continue\n",
        "                if \"http\" in m[1]:\n",
        "                    dataset[\"name\"] = m[0].strip()\n",
        "                    dataset[\"connection\"] = {\n",
        "                        \"name\": \"url\",\n",
        "                        \"source\": {\n",
        "                            \"url\": m[1].strip()\n",
        "                        }\n",
        "                    }\n",
        "                else:\n",
        "                    # local link to file in repo\n",
        "                    local_link = blob_link + \"/\" + m[1]\n",
        "                    dataset[\"name\"] = m[0].strip()\n",
        "                    dataset[\"connection\"] = {\n",
        "                        \"name\": \"url\",\n",
        "                        \"source\": {\n",
        "                            \"url\": local_link\n",
        "                        }\n",
        "                    }\n",
        "                datasets.append(dataset)\n",
        "    return datasets\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(detect_datasets(\"blah blah [mnist dataset](mnist) \\n WMT2016 English-Romanian\", \"\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'mnist dataset', 'connection': {'name': 'url', 'source': {'url': '/mnist'}}}, {'name': 'WMT2016 English-Romanian'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0VbLSAN21wY"
      },
      "source": [
        "import arxiv\n",
        "import re\n",
        "from dateutil import parser\n",
        "\n",
        "# http://arxiv.org/abs/1502.05698\n",
        "# https://arxiv.org/pdf/1512.03385.pdf\n",
        "\n",
        "ARXIV_ABS_PATTERN = \"https?:\\/\\/arxiv.org\\/abs\\/(\\d+\\.\\d+)\"\n",
        "ARXIV_PDF_PATTERN = \"https?:\\/\\/arxiv.org\\/pdf\\/(\\d+\\.\\d+)\\.pdf\"\n",
        "ARXIV_ID_PATTERN = \"arXiv:(\\d+\\.\\d+)\"\n",
        "\n",
        "# Given url, returns None if not arxiv or id if is\n",
        "def parse_arxiv_url(url):\n",
        "    arxiv_abs_re = re.compile(ARXIV_ABS_PATTERN)\n",
        "    arxiv_pdf_re = re.compile(ARXIV_PDF_PATTERN)\n",
        "\n",
        "    found = arxiv_abs_re.search(url)\n",
        "    if found:\n",
        "        return found.group(1)\n",
        "    found = arxiv_pdf_re.search(url)\n",
        "    if found:\n",
        "        return found.group(1)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Given full readme, look for arxiv and return info about each paper\n",
        "def look_for_arxiv_fulltext(text):\n",
        "    arxiv_abs_re = re.compile(ARXIV_ABS_PATTERN)\n",
        "    arxiv_pdf_re = re.compile(ARXIV_PDF_PATTERN)\n",
        "\n",
        "    ids = set()\n",
        "    found = arxiv_abs_re.findall(text)\n",
        "    if found and len(found) > 0:\n",
        "        for f in found:\n",
        "            ids.add(f)\n",
        "\n",
        "    found = arxiv_pdf_re.findall(text)\n",
        "    if found and len(found) > 0:\n",
        "        for f in found:\n",
        "            ids.add(f)\n",
        "\n",
        "    paper_info = []\n",
        "    for i in ids:\n",
        "        info = get_arxiv_id(i)\n",
        "        if info:\n",
        "            paper_info.append(info)\n",
        "    return paper_info\n",
        "\n",
        "# Given array of lines, look for arxiv and return info about each paper\n",
        "def look_for_arxiv(lines, start=0, end=None):\n",
        "    arxiv_abs_re = re.compile(ARXIV_ABS_PATTERN)\n",
        "    arxiv_pdf_re = re.compile(ARXIV_PDF_PATTERN)\n",
        "\n",
        "    if not end:\n",
        "        end = len(lines)-1\n",
        "\n",
        "    ids = set()\n",
        "    for i in range(start,end+1):\n",
        "        line = lines[i]\n",
        "        #print(line)\n",
        "        found = arxiv_abs_re.search(line)\n",
        "        if found:\n",
        "            ids.add(found.group(1))\n",
        "        found = arxiv_pdf_re.search(line)\n",
        "        if found:\n",
        "            ids.add(found.group(1))\n",
        "\n",
        "    paper_info = []\n",
        "    for i in ids:\n",
        "        info = get_arxiv_id(i)\n",
        "        if info:\n",
        "            paper_info.append(info)\n",
        "    return paper_info\n",
        "\n",
        "# Given text, look for arxiv and return info about paper\n",
        "def look_for_arxiv_id(text):\n",
        "    arxiv_re = re.compile(ARXIV_ID_PATTERN)\n",
        "    match = arxiv_re.search(text)\n",
        "    if match:\n",
        "        arxiv_id = match.group(1)\n",
        "        arxiv_info = get_arxiv_id(arxiv_id)\n",
        "        return arxiv_info\n",
        "    return None\n",
        "\n",
        "def get_arxiv_id(id):\n",
        "    results = arxiv.query(id_list=[id])\n",
        "    paper_info = None\n",
        "    # assume one result\n",
        "    if len(results) > 0:\n",
        "        result = results[0]\n",
        "        published = parser.parse(result.published)\n",
        "        paper_info = {\n",
        "            \"title\": result.title,\n",
        "            \"authors\": result.authors,\n",
        "            \"arxiv\": id,\n",
        "            \"year\": published.year,\n",
        "            \"url\": result.arxiv_url\n",
        "        }\n",
        "        if \"summary\" in result:\n",
        "            paper_info[\"abstract\"] = result.summary\n",
        "    return paper_info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CDC7PHf3AmV"
      },
      "source": [
        "import re\n",
        "import bibtexparser\n",
        "\n",
        "REF_PATTERNS = {\n",
        "    # IBM MAX 1, ex:* _S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen,\\nR. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et  al._,\\n[\"CNN architectures for large-scale audio classification,\"](https://arxiv.org/pdf/1609.09430.pdf) arXiv preprint\\narXiv:1609.09430, 2016.\n",
        "    '\\* _([A-Za-z,\\-\\.\\s]+?)_\\s?,\\s?\\[\"?(.+?)\"?\\]\\((\\S+?)\\)\\s?arXiv preprint\\s?(arXiv:\\d+\\.\\d+), (\\d+)\\.': [\"authors\", \"title\", \"url\", \"arxiv\", \"year\"],\n",
        "    # IBM MAX 2, ex:* _Qiuqiang Kong, Yong Xu, Wenwu Wang, Mark D. Plumbley_,[\"Audio Set classification with attention model: A probabilistic perspective.\"](https://arxiv.org/pdf/1711.00927.pdf) arXiv preprint arXiv:1711.00927 (2017).\n",
        "    '\\* _([A-Za-z,\\-\\.\\s]+?)_\\s?,\\s?\\[\"?(.+?)\"?\\]\\((\\S+?)\\)\\s?arXiv preprint\\s?(arXiv:\\d+\\.\\d+) \\((\\d+)\\)\\.': [\"authors\", \"title\", \"url\", \"arxiv\", \"year\"],\n",
        "    # IBM MAX 3, ex:* _Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter_,[\"Audio set: An ontology and human-labeled dataset for audio events\"](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf), IEEE ICASSP, 2017.\n",
        "    '\\* _([A-Za-z,\\-\\.\\s]+?)_\\s?,\\s?\\[\"?(.+?)\"?\\]\\((\\S+?)\\)(?:\\.|,) ([\\s\\S]+?), (\\d+)\\.': [\"authors\", \"title\", \"url\", \"venue\", \"year\"],\n",
        "    # IBM MAX 4, ex:[1]<a name=\"ref1\"></a> N. Shazeer, R. Doherty, C. Evans, C. Waterson., [\"Swivel: Improving Embeddings\\nby Noticing What's Missing\"](https://arxiv.org/pdf/1602.02215.pdf) arXiv preprint arXiv:1602.02215 (2016)\n",
        "    '\\[\\d+\\]<a (?:.*)><\\/a> ([A-Za-z,\\-\\.\\s]+?)\\s?\\[\"?([\\s\\S]+?)\"?\\]\\((\\S+?)\\)\\s?arXiv preprint\\s?(arXiv:\\d+\\.\\d+) \\((\\d+)\\)': [\"authors\", \"title\", \"url\", \"arxiv\", \"year\"],\n",
        "    # IBM MAX 5, ex:[1] Jaderberg, Max, et al. [\"Spatial Transformer Networks\"](https://arxiv.org/pdf/1506.02025) arXiv preprint arXiv:1506.02025 (2015)\n",
        "    '\\[\\d+\\]\\s?([A-Za-z,\\-\\.\\s]+?)\\s?\\[\"?([\\s\\S]+?)\"?\\]\\((\\S+?)\\)\\s?arXiv preprint\\s?(arXiv:\\d+\\.\\d+) \\((\\d+)\\)': [\"authors\", \"title\", \"url\", \"arxiv\", \"year\"],\n",
        "    # IBM MAX 6, ex:* _D. Tran, L. Bourdev, R. Fergus, L. Torresani, M. Paluri_, [C3D: Generic Features for Video Analysis](http://vlg.cs.dartmouth.edu/c3d/)\n",
        "    '\\* _([A-Za-z,\\-\\.\\s]+?)_\\s?,\\s?\\[\"?(.+?)\"?\\]\\((\\S+?)\\)': [\"authors\", \"title\", \"url\"],\n",
        "    # IBM MAX 7, ex:* [Sports-1M Dataset Project Page](https://cs.stanford.edu/people/karpathy/deepvideo/)\n",
        "    '\\* \\[\"?(.+?)\"?\\]\\((\\S+?)\\)': [\"title\", \"url\"],\n",
        "    # IBM MAX 8, ex: Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer: “Exploring the Limits of Language Modeling”, 2016;\\n[arXiv:1602.02410](http://arxiv.org/abs/1602.02410).\n",
        "    '^([A-Za-z,\\-\\.\\s]+?)\\s?:\\s?\"?(.+?)\"?, (\\d+);\\s?\\[\"?(.+?)\"?\\]\\((\\S+?)\\)\\.': [\"authors\", \"title\", \"year\", \"arxiv\", \"url\"],\n",
        "    # GitHub, ex:  - Raissi, Maziar, Paris Perdikaris, and George Em Karniadakis. \"[Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1711.10561).\" arXiv preprint arXiv:1711.10561 (2017).\n",
        "    '\\s*?-\\s?([A-Za-z,\\-\\.\\s]+?).\\s?\"\\[\"?(.+?)\"?\\]\\((\\S+?)\\).\"\\s?arXiv preprint\\s?(arXiv:\\d+\\.\\d+)\\s?\\((\\d+?)\\)\\.': [\"authors\", \"title\", \"url\", \"arxiv\", \"year\"],\n",
        "}\n",
        "URL_PATTERN = \"^(?:http(s)?:\\/\\/)?[\\w.-]+(?:\\.[\\w\\.-]+)+[\\w\\-\\._~:/?#[\\]@!\\$&'\\(\\)\\*\\+,;=.]+$\"\n",
        "URL_ENDER_EXCEPTIONS = (\".json\", \".yaml\", \".py\", \".ipynb\")\n",
        "ANCHOR_PATTERN = \"^#\\S+\"\n",
        "CODEBLOCK_PATTERN = \"\\s+?```(?:bibtex|)([\\s\\S]+?)```\"\n",
        "\n",
        "def make_paper_key(ref_info):\n",
        "    title = ref_info[\"title\"] if \"title\" in ref_info else \"\"\n",
        "    authors = \"\"\n",
        "    if \"authors\" in ref_info:\n",
        "        if isinstance(ref_info[\"authors\"], list):\n",
        "            for a in ref_info[\"authors\"]:\n",
        "                authors += a\n",
        "        elif isinstance(ref_info[\"authors\"], str):\n",
        "            authors = ref_info[\"authors\"]\n",
        "    return title + authors\n",
        "\n",
        "def detect_references(readme):\n",
        "    found = {}\n",
        "\n",
        "    arxiv_ids = set()\n",
        "    arxiv_papers = look_for_arxiv_fulltext(readme)\n",
        "\n",
        "    for a in arxiv_papers:\n",
        "        arxiv_ids.add(a[\"arxiv\"])\n",
        "        paper_key = make_paper_key(a)\n",
        "        found[paper_key] = a\n",
        "\n",
        "    bibtex_papers = codeblock_search(readme)\n",
        "    for b in bibtex_papers:\n",
        "        # skip if arxiv already found\n",
        "        if \"arxiv\" in b:\n",
        "            if b[\"arxiv\"] in arxiv_ids:\n",
        "                continue\n",
        "            if \"arxiv:\" in b[\"arxiv\"].lower():\n",
        "                arxiv_id = b[\"arxiv\"][b[\"arxiv\"].lower().find(\"arxiv:\")+len(\"arxiv:\"):]\n",
        "                if arxiv_id in arxiv_ids:\n",
        "                    continue\n",
        "        paper_key = make_paper_key(b)\n",
        "        if paper_key not in found:\n",
        "            found[paper_key] = b\n",
        "        else:\n",
        "            if len(b.keys()) > len(found[paper_key].keys()):\n",
        "                found[paper_key] = b\n",
        "\n",
        "    regex_papers = regex_search(readme)\n",
        "    for r in regex_papers:\n",
        "        # skip entire ref if arxiv already found\n",
        "        if \"arxiv\" in r:\n",
        "            if r[\"arxiv\"] in arxiv_ids:\n",
        "                continue\n",
        "            if \"arxiv:\" in r[\"arxiv\"].lower():\n",
        "                arxiv_id = r[\"arxiv\"][r[\"arxiv\"].lower().find(\"arxiv:\")+len(\"arxiv:\"):]\n",
        "                if arxiv_id in arxiv_ids:\n",
        "                    continue\n",
        "        if \"url\" in r:\n",
        "            arxiv_id = parse_arxiv_url(r[\"url\"])\n",
        "            if arxiv_id and arxiv_id in arxiv_ids:\n",
        "                continue\n",
        "        paper_key = make_paper_key(r)\n",
        "        # give priority to arxiv\n",
        "        if paper_key not in found:\n",
        "            found[paper_key] = r\n",
        "        else:\n",
        "            if len(r.keys()) > len(found[paper_key].keys()):\n",
        "                found[paper_key] = r\n",
        "\n",
        "    if len(found.values()) > 0:\n",
        "        refs = []\n",
        "        for ref in found.values():\n",
        "            if \"authors\" in ref and isinstance(ref[\"authors\"], str):\n",
        "                ref[\"authors\"] = ref[\"authors\"].split(\",\")\n",
        "            refs.append(ref)\n",
        "        return {\n",
        "            \"references\": refs\n",
        "        }\n",
        "\n",
        "    return {}\n",
        "\n",
        "def regex_search(readme):\n",
        "    ref_res = []\n",
        "    for r in REF_PATTERNS.keys():\n",
        "        ref_res.append(re.compile(r, re.M))\n",
        "\n",
        "    found_refs = []\n",
        "    for r in ref_res:\n",
        "        result = r.findall(readme)\n",
        "        headers = REF_PATTERNS[r.pattern]\n",
        "        for match in result:\n",
        "            ref = {}\n",
        "            for i in range(len(headers)):\n",
        "                header = headers[i]\n",
        "                ref[header] = match[i]\n",
        "            if \"url\" in ref:\n",
        "                # skip if not actual URL\n",
        "                url_re = re.compile(URL_PATTERN)\n",
        "                urls = url_re.search(ref[\"url\"])\n",
        "                if not urls:\n",
        "                    continue\n",
        "                # skip if URL ends with certain strings\n",
        "                if ref[\"url\"].endswith(URL_ENDER_EXCEPTIONS):\n",
        "                    continue\n",
        "            # skip entire ref if url is an anchor (i.e. just #)\n",
        "                anchor_re = re.compile(ANCHOR_PATTERN)\n",
        "                anchors = anchor_re.search(ref[\"url\"])\n",
        "                if anchors:\n",
        "                    continue\n",
        "\n",
        "            found_refs.append(ref)\n",
        "    return found_refs\n",
        "\n",
        "def codeblock_search(readme):\n",
        "    found_refs = []\n",
        "    codeblock_re = re.compile(CODEBLOCK_PATTERN, re.M)\n",
        "    result = codeblock_re.findall(readme)\n",
        "    for match in result:\n",
        "        #print(\"regex match\", match)\n",
        "        bib_results = bibtexparser.loads(match)\n",
        "        if len(bib_results.entries) > 0:\n",
        "            for e in bib_results.entries:\n",
        "                ref = bibtexparser.customization.author(e)\n",
        "                ref[\"authors\"] = ref[\"author\"]\n",
        "                if \"journal\" in ref:\n",
        "                    arxiv_info = look_for_arxiv_id(ref[\"journal\"])\n",
        "                    if arxiv_info:\n",
        "                        # merge, give preference to whatever is longer\n",
        "                        for k,v in arxiv_info.items():\n",
        "                            if k == \"arxiv\":\n",
        "                                ref[k] = v\n",
        "                            try:\n",
        "                                if k in ref:\n",
        "                                    if len(v) > len(ref[k]):\n",
        "                                        ref[k] = v\n",
        "                                    else:\n",
        "                                        ref[k] = v\n",
        "                            except TypeError:\n",
        "                                ref[k] = v\n",
        "                found_refs.append(ref)\n",
        "\n",
        "    return found_refs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttgDJA5k3I0B"
      },
      "source": [
        "from github3 import login, GitHubEnterprise, exceptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LkEDHD43UKP"
      },
      "source": [
        "def value_json_to_schema(schema_contents):\n",
        "    try:\n",
        "        value_info = json.loads(schema_contents)\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        # This is to catch the case where value_info.json is stored in LFS\n",
        "        print(e)\n",
        "        return None\n",
        "    #print(value_info)\n",
        "    dimensions = None\n",
        "    for k,v in value_info.items():\n",
        "        if \"data\" in k:\n",
        "            dimensions = v[1]\n",
        "    input_schema = None\n",
        "    if dimensions:\n",
        "        input_schema = {\n",
        "            \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n",
        "            \"$id\": \"input_definition_data_schema.json\",\n",
        "            \"id\": \"input_definition_data_schema.json\",\n",
        "            \"title\": \"Input Data Schema from value_info.json\",\n",
        "            \"type\": \"object\",\n",
        "            \"required\": [\"input\"],\n",
        "            \"properties\": {\n",
        "                \"input\": {\n",
        "                  \"description\": \"Images\",\n",
        "                  \"dimensions\": dimensions\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    return input_schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxFppKG935De"
      },
      "source": [
        "\"\"\"\n",
        "    Utility functions used by the various parsers.\n",
        "\"\"\"\n",
        "\n",
        "from github3.exceptions import NotFoundError\n",
        "import base64, json, yaml\n",
        "\n",
        "BINARY_EXTS = (\".checkpoint\", \"Dockerfile\", \".caffemodel\", \".pb\", \".pbtxt\",\n",
        "    \".prototxt\", \".ckpt\", \".meta\", \".index\", \".onnx\", \".joblib\", \",pkl\", \".h5\",\n",
        "    \".hdf5\", \"value_info.json\")\n",
        "\n",
        "# Expects repo object from github.repo, returns None if not found\n",
        "def get_file_from_repo(repo_object, filepath):\n",
        "    file_contents = None\n",
        "    try:\n",
        "        file_contents = repo_object.file_contents(filepath)\n",
        "    except NotFoundError as e:\n",
        "        return None\n",
        "    if file_contents is not None:\n",
        "        #contents = file_contents.decoded\n",
        "        contents = file_contents.content\n",
        "        if file_contents.encoding == \"base64\":\n",
        "            contents = base64.b64decode(contents)\n",
        "            contents = contents.decode('UTF-8')\n",
        "        return contents\n",
        "\n",
        "# slightly more complicated dict merger that is subobject aware\n",
        "# for conflicts, dict2 takes priority\n",
        "def merge_metadata(dict1, dict2):\n",
        "    SUBOBJECTS = [\"definition\", \"training\", \"trained_model\", \"provenance\"]\n",
        "    result = {**dict1, **dict2}\n",
        "    for s in SUBOBJECTS:\n",
        "        if s in dict1 and s in dict2:\n",
        "            result[s] = {**dict1[s], **dict2[s]}\n",
        "            # check for inner dicts\n",
        "            for k, v in dict1[s].items():\n",
        "                if k in dict2[s] and isinstance(v, dict):\n",
        "                    result[s][k] = {**dict1[s][k], **dict2[s][k]}\n",
        "    # special case of evaluations\n",
        "    if \"evaluations\" in dict1 and \"evaluations\" in dict2:\n",
        "        result[\"evaluations\"] = dict1[\"evaluations\"]\n",
        "        for ev in dict2[\"evaluations\"]:\n",
        "            result[\"evaluations\"].append(ev)\n",
        "    # special note of authors: 2nd dict takes priority in list order\n",
        "    if \"authors\" in dict2 and \"authors\" in dict1:\n",
        "        result[\"authors\"] = dict1[\"authors\"]\n",
        "        for a in dict2[\"authors\"]:\n",
        "            result[\"authors\"].append(a)\n",
        "    if \"trained_model\" in dict1 and \"trained_model\" in dict2:\n",
        "        if \"binaries\" in dict1[\"trained_model\"] and \"binaries\" in dict2[\"trained_model\"]:\n",
        "            result[\"trained_model\"][\"binaries\"] = dict1[\"trained_model\"][\"binaries\"] + dict2[\"trained_model\"][\"binaries\"]\n",
        "    return result\n",
        "\n",
        "def get_blob_link(repo_object, tree_path=None, branch_name=None):\n",
        "    if not branch_name:\n",
        "        branch = repo_object.default_branch\n",
        "    else:\n",
        "        branch = branch_name\n",
        "    url = repo_object.html_url\n",
        "    if tree_path:\n",
        "        folders = \"/\".join(tree_path)\n",
        "        url += \"/blob/\" + branch + \"/\" + folders\n",
        "    else:\n",
        "        url += \"/blob/\" + branch\n",
        "    return url\n",
        "\n",
        "def get_tree_link(repo_object, tree_path=None, branch_name=None):\n",
        "    if not branch_name:\n",
        "        branch = repo_object.default_branch\n",
        "    else:\n",
        "        branch = branch_name\n",
        "    url = repo_object.html_url\n",
        "    if tree_path:\n",
        "        folders = \"/\".join(tree_path)\n",
        "        url += \"/tree/\" + branch + \"/\" + folders\n",
        "    else:\n",
        "        url += \"/tree/\" + branch\n",
        "    return url\n",
        "\n",
        "# returns all files in folder in tuples of (path, type, blob/tree)\n",
        "# NOTE: only the first tree (at least for now, due to performance reasons)\n",
        "def get_all_files_from_folder(repo_object, tree_path=None, exceptions=[]):\n",
        "    if tree_path:\n",
        "        folder_name = \"/\".join(tree_path)\n",
        "    else:\n",
        "        folder_name = \"/\"\n",
        "    contents = repo_object.directory_contents(folder_name)\n",
        "    file_contents = []\n",
        "    for c in contents:\n",
        "        try:\n",
        "            if c[1].type == \"dir\" and c[0] not in exceptions:\n",
        "                file_contents.append( (c[0], \"dir\", repo_object.tree(c[1].sha)) )\n",
        "            else:\n",
        "                file_contents.append( (c[0], \"file\", repo_object.blob(c[1].sha)) )\n",
        "        except NotFoundError as e:\n",
        "            # NOTE: seems to trigger for submodules, for now, go ahead and skip\n",
        "            print(e)\n",
        "            continue\n",
        "    return file_contents\n",
        "\n",
        "# given path and metadata_object, traverses object and inserts value into path\n",
        "# to_object converts the value into an object\n",
        "def path_to_object(metadata_object, value, path, to_object=False, is_yaml=False):\n",
        "    path_list = path.split(\"/\")\n",
        "    # ensure each object before the last part of the path is created\n",
        "    current_object = metadata_object\n",
        "    for i in range(len(path_list)):\n",
        "        path_part = path_list[i]\n",
        "\n",
        "        if i < len(path_list) - 1:\n",
        "            if path_part not in current_object:\n",
        "                current_object[path_part] = {}\n",
        "            current_object = current_object[path_part]\n",
        "        else:\n",
        "            # last part path\n",
        "            if to_object:\n",
        "                if is_yaml:\n",
        "                    value = yaml.safe_load(value)\n",
        "                else:\n",
        "                    value = json.loads(value)\n",
        "            current_object[path_part] = value\n",
        "    return metadata_object\n",
        "\n",
        "def is_binary_ext(path):\n",
        "    if path.endswith(BINARY_EXTS):\n",
        "        return True\n",
        "    return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlm8W8B-4EQy"
      },
      "source": [
        "from os.path import join, dirname\n",
        "from github3 import login, GitHubEnterprise, exceptions\n",
        "import base64, re, os, json\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown, Markdown\n",
        "from io import StringIO\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "    #print(\"html\", html)\n",
        "    # remove code snippets\n",
        "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
        "    html = re.sub(r'<code>(.*?)</code>', ' ', html)\n",
        "    html = re.sub(r'<img>(.*?)</img>', ' ', html)\n",
        "    html = re.sub(r'<a>(.*?)</a>', ' ', html)\n",
        "    #print(\"html\", html)\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    # print(\"soup\", soup)\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "    # print(\"text\", text)\n",
        "    return text\n",
        "\n",
        "def unmark_element(element, stream=None):\n",
        "    if stream is None:\n",
        "        stream = StringIO()\n",
        "    if element.text:\n",
        "        stream.write(element.text)\n",
        "    for sub in element:\n",
        "        unmark_element(sub, stream)\n",
        "    if element.tail:\n",
        "        stream.write(element.tail)\n",
        "    return stream.getvalue()\n",
        "\n",
        "Markdown.output_formats[\"plain\"] = unmark_element\n",
        "__md = Markdown(output_format=\"plain\")\n",
        "__md.stripTopLevelTags = False\n",
        "\n",
        "def unmark(text):\n",
        "    return __md.convert(text)\n",
        "\n",
        "def markdownToText(md):\n",
        "\n",
        "    md = md.strip()\n",
        "    # double clena up with 2 different approaches:\n",
        "    # 1. html markdown\n",
        "    text = markdown_to_text(md)\n",
        "    # 2. by text element\n",
        "    text = unmark(text)\n",
        "    # remove duplicate symbols\n",
        "    text = re.sub(r'(=|:|\\t|\\\"|_|-){2,}', \"\", text)\n",
        "    # remove empty lines\n",
        "    # plain = re.sub(r'^\\s*$', \"\", plain)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text, re.MULTILINE)\n",
        "    # remove URL\n",
        "    # text = re.sub(r'\\(?https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\\)?', '', text)\n",
        "    text = re.sub(r'\\(?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\)?', '', text)\n",
        "    #remove email\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "    # replace consecutives chars by a single one\n",
        "    text = re.sub(r'(\\s|\\n)\\1+', r'\\1', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def readme_cleanup(markdown_readme):\n",
        "    plain_readme = markdownToText(markdown_readme)\n",
        "\n",
        "    return plain_readme\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUvXyJg44KS5"
      },
      "source": [
        "\"\"\"\n",
        "    Given a list of special files, looks for them and tries to parse them for metadata.\n",
        "    Also supports yaml equivalents.\n",
        "\n",
        "    Currently supported files:\n",
        "        input_definition_data_schema.json / input_data_schema.json\n",
        "        output_definition_data_schema.json / output_data_schema.json\n",
        "        hyperparameter_schema.json\n",
        "        hyperparameters.json\n",
        "        input_trained_data_schema.json\n",
        "        output_trained_data_schema.json\n",
        "        features.json\n",
        "        topology.json\n",
        "        pipeline.json\n",
        "\"\"\"\n",
        "\n",
        "# from .gh_utils import get_file_from_repo, merge_metadata, path_to_object\n",
        "\n",
        "SPECIAL_FILES = {\n",
        "    \"input_definition_data_schema\": \"definition/input_data_schema\",\n",
        "    \"input_data_schema\": \"definition/input_data_schema\",\n",
        "    \"output_definition_data_schema\": \"definition/output_data_schema\",\n",
        "    \"output_data_schema\": \"definition/output_data_schema\",\n",
        "    \"hyperparameter_schema\": \"definition/hyperparameter_schema\",\n",
        "    \"topology\": \"definition/topology\",\n",
        "    \"hyperparameters\": \"training/hyperparameters\",\n",
        "    \"features\": \"training/features\",\n",
        "    \"input_trained_data_schema\": \"trained_model/input_data_schema\",\n",
        "    \"output_trained_data_schema\": \"trained_model/output_data_schema\",\n",
        "    \"pipeline\": \"pipeline\"\n",
        "}\n",
        "\n",
        "def is_special_file(filename):\n",
        "    for special_file_names in SPECIAL_FILES.keys():\n",
        "        if filename.endswith(special_file_names + \".json\") or filename.endswith(special_file_names + \".yaml\"):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_special_file(repo_object, filepath, propertypath, metadata, is_yaml=False):\n",
        "    contents = get_file_from_repo(repo_object, filepath)\n",
        "    if contents is None:\n",
        "        return metadata\n",
        "    return path_to_object(metadata, contents, propertypath, to_object=True, is_yaml=is_yaml)\n",
        "\n",
        "def detect_special_files(repo_object, tree_path=None):\n",
        "    result = {}\n",
        "\n",
        "    for k, v in SPECIAL_FILES.items():\n",
        "        if tree_path:\n",
        "            folders = \"/\".join(tree_path)\n",
        "            json_file = folders + \"/\" + k + \".json\"\n",
        "            yaml_file = folders + \"/\" + k + \".yaml\"\n",
        "        else:\n",
        "            json_file = k + \".json\"\n",
        "            yaml_file = k + \".yaml\"\n",
        "        path = v\n",
        "        result = get_special_file(repo_object, json_file, path, result)\n",
        "        result = get_special_file(repo_object, yaml_file, path, result, is_yaml=True)\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRFdfORP4PX4"
      },
      "source": [
        "from github3.exceptions import NotFoundError\n",
        "import base64, re\n",
        "LINK_PATTERN = \"\\[([^\\]]+?)\\]\\(([^\\)]+?)\\)\"\n",
        "ANCHOR_PATTERN = \"^#\\S+\"\n",
        "\n",
        "MODEL_HEADER_PATTERN = \"## Model(?:s| Metadata)\\s+?([\\s\\S]+?)\\s+?(?:#|\\Z)\"\n",
        "MODEL_VALUE_HEADER_PATTERN = \"##\\s+?Model Value\\n+?((?:###\\s+?(?:[^#]+?)\\n(?:^\\|[\\S\\s]+?\\|\\n{2,}))*)\"\n",
        "TABLE_PATTERN = \"###\\s+?([^#]+?)\\n(^\\|[\\S\\s]+?\\|\\n{2,})\"\n",
        "\n",
        "MODEL_METRIC_SPECIAL_CASE = \"Test Set\"\n",
        "\n",
        "SCHEMA_HEADER_PATTERN_PRE = \"##\\s*?\"\n",
        "SCHEMA_HEADER_PATTERN_POST = \"\\s+?```(?:json|)([\\s\\S]+?)```\"\n",
        "\n",
        "TABLE_HEADER_PATTERN_PRE = \"##\\s*?\"\n",
        "TABLE_HEADER_PATTERN_POST = \"\\s+?(^\\|[\\S\\s]+?\\|\\n{2,})\"\n",
        "\n",
        "CODE_BLOCK_HEADERS_DICT = {\n",
        "    \"Input(?: Definition| ) Data Schema\": \"definition/input_data_schema\",\n",
        "    \"Output(?: Definition| ) Data Schema\": \"definition/output_data_schema\",\n",
        "    \"Hyperparameter Schema\": \"definition/hyperparameter_schema\",\n",
        "    \"Input Trained Data Schema\": \"trained_model/input_data_schema\",\n",
        "    \"Output Trained Data Schema\": \"trained_model/output_data_schema\"\n",
        "}\n",
        "\n",
        "README_FILES = [\"README.md\", \"README\", \"readme.md\", \"readme\"]\n",
        "#INPUT_TRAINED_SCHEMA_HEADER = \"## Input Trained Data Schema\\s+?```(?:json)([\\s\\S]+?)```\\s+?(?:#|\\Z)\"\n",
        "\n",
        "def is_readme_file(filename):\n",
        "    for readme_file_names in README_FILES:\n",
        "        if filename.endswith(readme_file_names):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_readme_contents(repo_object):\n",
        "    try:\n",
        "        readme = repo_object.readme()\n",
        "    except NotFoundError as e:\n",
        "        return None\n",
        "    contents = readme.content\n",
        "    if readme.encoding == \"base64\":\n",
        "        contents = base64.b64decode(contents)\n",
        "        contents = contents.decode('UTF-8')\n",
        "    return contents\n",
        "\n",
        "def get_readme_contents_from_sha(repo_object, sha):\n",
        "    readme_blob = repo_object.blob(readme_sha)\n",
        "    readme = readme_blob.decode_content()\n",
        "    return readme\n",
        "\n",
        "def get_readme_contents_from_path(repo_object, tree_path):\n",
        "    full_path = \"\"\n",
        "    for folder in tree_path:\n",
        "        full_path += folder +\"/\"\n",
        "    readme = None\n",
        "    for readme_names in README_FILES:\n",
        "        readme_path = full_path + readme_names\n",
        "        readme = get_file_from_repo(repo_object, readme_path)\n",
        "        if readme:\n",
        "            break\n",
        "    return readme\n",
        "\n",
        "def get_readme_contents_from_docstring(repo_object, sha):\n",
        "    code_blob = repo_object.blob(sha)\n",
        "    code = code_blob.decode_content()\n",
        "    # only read the docstring ('''....''')\n",
        "    start = None\n",
        "    end = None\n",
        "    lines = code.split(\"\\n\")\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        #print(line)\n",
        "        if line.startswith(\"'''\") or line.startswith('\"\"\"'):\n",
        "            if start is None:\n",
        "                start = i\n",
        "            elif end is None:\n",
        "                end = i\n",
        "                break\n",
        "    if start is None or end is None:\n",
        "        return None\n",
        "    docstring = \"\"\n",
        "    for i in range(start,end+1):\n",
        "        line = lines[i].strip()\n",
        "        # remove start of docstring\n",
        "        if i == start:\n",
        "            docstring_start = line.find(\"'''\")\n",
        "            if docstring_start == -1:\n",
        "                docstring_start = line.find('\"\"\"')\n",
        "            line = line[docstring_start+3:]\n",
        "        # remove end of docstring\n",
        "        if i == end:\n",
        "            docstring_end = line.find(\"'''\")\n",
        "            if docstring_end == -1:\n",
        "                docstring_end = line.find('\"\"\"')\n",
        "            line = line[:docstring_end]\n",
        "        docstring += line + \"\\n\"\n",
        "    return docstring\n",
        "\n",
        "def get_readme_title(readme):\n",
        "    link_re = re.compile(LINK_PATTERN)\n",
        "\n",
        "    # Use readme title as model name\n",
        "    lines = readme.splitlines()\n",
        "    line_index = 0\n",
        "    title = None\n",
        "    while line_index < len(lines):\n",
        "        line = lines[line_index].strip()\n",
        "        skip = False\n",
        "\n",
        "        if line == \"\":\n",
        "            skip = True\n",
        "\n",
        "        # skip cases\n",
        "        if line.lower() == \"introduction\" or line.lower().startswith(\"*\"):\n",
        "            skip = True\n",
        "\n",
        "        # special cases\n",
        "        if \"travis-ci\" in line.lower():\n",
        "            skip = True\n",
        "\n",
        "        # special RST cases\n",
        "        if line.lower().startswith(\".. \") or line.lower().startswith(\":target:\"):\n",
        "            skip = True\n",
        "\n",
        "        # another special RST case\n",
        "        rst_table_re = re.compile(\"^(\\|[^\\|]+?\\|_\\s*)+\")\n",
        "        matches = rst_table_re.search(line)\n",
        "        if matches:\n",
        "            skip = True\n",
        "\n",
        "        # skip if line is a link and image\n",
        "        link_image_re = re.compile(\"^\\[!\\[([^\\]]+?)\\]\\(([^\\)]+?)\\)\\]\\(([^\\)]+?)\\)\")\n",
        "        matches = link_image_re.search(line)\n",
        "        if matches:\n",
        "            skip = True\n",
        "\n",
        "        # skip if line is an Image\n",
        "        image_re = re.compile(\"^!\\[([^\\]]+?)\\]\\(([^\\)]+?)\\)\")\n",
        "        matches = image_re.search(line)\n",
        "        if matches:\n",
        "            skip = True\n",
        "\n",
        "        # skip if line is nothing but \"-\" or \"=\" or \"*\" or \"_\"\n",
        "        lines_re = re.compile(\"^[-=\\*_]+\")\n",
        "        matches = lines_re.search(line)\n",
        "        if matches:\n",
        "            skip = True\n",
        "\n",
        "        if not skip:\n",
        "            title = line\n",
        "            # remove hashes in front and/or back if they exist\n",
        "            title_re = re.compile(\"^#+ ([^#]+)#*$\")\n",
        "            matches = title_re.search(title)\n",
        "            if (matches):\n",
        "                title = matches.group(1).strip()\n",
        "\n",
        "            # remove <tags> if they exist\n",
        "            tags_re = re.compile(\"<[\\S\\s]+?>\")\n",
        "            matches = tags_re.findall(title)\n",
        "            if (matches and len(matches) > 0):\n",
        "                for m in matches:\n",
        "                    title = title.replace(m, \"\")\n",
        "\n",
        "            # if a link is in the title, replace it\n",
        "            links = link_re.search(title)\n",
        "            if links:\n",
        "                title = title.replace(links.group(0), links.group(1))\n",
        "\n",
        "            break\n",
        "\n",
        "        line_index += 1\n",
        "\n",
        "    return title\n",
        "\n",
        "def get_dataset_links(readme, blob_link):\n",
        "    link_re = re.compile(LINK_PATTERN)\n",
        "\n",
        "    # extract out links and look for \"dataset or data set\" and an actual link\n",
        "    matches = link_re.findall(readme)\n",
        "    datasets = {}\n",
        "    if (matches and len(matches) > 0):\n",
        "        for m in matches:\n",
        "            if \"dataset\" in m[0].lower() or \"data set\" in m[0].lower() or \"corpus\" in m[0].lower():\n",
        "                # ignore if anchor\n",
        "                anchor_re = re.compile(ANCHOR_PATTERN)\n",
        "                anchors = anchor_re.search(m[1])\n",
        "                if anchors:\n",
        "                    continue\n",
        "                if \"http\" in m[1]:\n",
        "                    datasets[m[0]] = m[1]\n",
        "                else:\n",
        "                    # local link to file in repo\n",
        "                    local_link = blob_link + \"/\" + m[1]\n",
        "                    datasets[m[0]] = local_link\n",
        "    return datasets\n",
        "\n",
        "def check_codeblock(readme, header, metadata_path, metadata, to_object=False):\n",
        "    header_pattern = SCHEMA_HEADER_PATTERN_PRE + header + SCHEMA_HEADER_PATTERN_POST\n",
        "    header_re = re.compile(header_pattern, re.M | re.I)\n",
        "\n",
        "    result = header_re.search(readme)\n",
        "    if (result and len(result.groups()) > 0):\n",
        "        code_block = result.group(1).strip()\n",
        "        block_result = path_to_object({}, code_block, metadata_path, to_object)\n",
        "        return merge_metadata(metadata, block_result)\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def check_table(readme, header, metadata_path, metadata, header_value_dict=None):\n",
        "    header_pattern = TABLE_HEADER_PATTERN_PRE + header + TABLE_HEADER_PATTERN_POST\n",
        "    header_re = re.compile(header_pattern, re.M | re.I)\n",
        "\n",
        "    result = header_re.search(readme)\n",
        "    if (result and len(result.groups()) > 0):\n",
        "        table_block = result.group(1).strip()\n",
        "        table_result = parse_markdown_table(table_block, header_value_dict)\n",
        "        if len(table_result) > 0:\n",
        "            table_result = path_to_object(metadata, table_result, metadata_path)\n",
        "        return table_result\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# returns array of objects, optionally can define paths\n",
        "def parse_markdown_table(metric_table_md, header_value_dict=None):\n",
        "    rows = metric_table_md.split(\"\\n\")\n",
        "    headers = rows[0].split(\"|\")\n",
        "    metadata_objects = []\n",
        "    for r_index in range(2, len(rows)):\n",
        "        metadata = None\n",
        "        values = rows[r_index].split(\"|\")\n",
        "        for i in range(len(headers)):\n",
        "            h = headers[i].strip()\n",
        "            v = values[i].strip()\n",
        "\n",
        "            if len(h) > 0:\n",
        "                if not metadata:\n",
        "                    metadata = {}\n",
        "                if header_value_dict:\n",
        "                    if h.lower() in header_value_dict:\n",
        "                        metadata = path_to_object(metadata, v, header_value_dict[h.lower()])\n",
        "                else:\n",
        "                    metadata[h.lower()] = v\n",
        "\n",
        "        if metadata:\n",
        "            metadata_objects.append(metadata)\n",
        "\n",
        "    return metadata_objects\n",
        "\n",
        "def check_model_value_table(readme):\n",
        "    metadata = {}\n",
        "    header_re = re.compile(MODEL_VALUE_HEADER_PATTERN, re.M | re.I)\n",
        "    result = header_re.search(readme)\n",
        "    evaluations = []\n",
        "    if (result and len(result.groups()) > 0):\n",
        "        # add newlines for the regex pattern matching\n",
        "        raw_table = result.group(1).strip() + \"\\n\\n\"\n",
        "        table_re = re.compile(TABLE_PATTERN, re.M | re.I)\n",
        "        results = table_re.findall(raw_table)\n",
        "        metric_metadata = {\n",
        "            \"evaluation_type\": \"training_evaluation\",\n",
        "            \"method\": MODEL_METRIC_SPECIAL_CASE\n",
        "        }\n",
        "        metrics = []\n",
        "        for r in results:\n",
        "            metric_name = r[0]\n",
        "            table_md = r[1].strip()\n",
        "            parse_result = {\n",
        "                \"metric\": metric_name,\n",
        "            }\n",
        "            # special case\n",
        "            if metric_name.lower() == \"cost of training\":\n",
        "                parse_result = parse_markdown_table(table_md)\n",
        "                if len(parse_result) > 0:\n",
        "                    metadata[\"training\"] = {\n",
        "                        \"training_job\": {\n",
        "                            \"costs\": parse_result\n",
        "                        }\n",
        "                    }\n",
        "            else:\n",
        "                parse_results = parse_markdown_table(table_md)\n",
        "                comparisons = []\n",
        "                for pr in parse_results:\n",
        "                    # special case that defines what the metric is\n",
        "                    if \"above\" in pr and pr[\"above\"].lower() == MODEL_METRIC_SPECIAL_CASE.lower():\n",
        "                        parse_result[\"value\"] = pr[\"measurement\"]\n",
        "                    else:\n",
        "                        comparisons.append(pr)\n",
        "                if len(comparisons) > 0:\n",
        "                    parse_result[\"comparisons\"] = comparisons\n",
        "                metrics.append(parse_result)\n",
        "        if len(metrics) > 0:\n",
        "            metric_metadata[\"metrics\"] = metrics\n",
        "\n",
        "        evaluations.append(metric_metadata)\n",
        "    if len(evaluations) > 0:\n",
        "        metadata[\"evaluations\"] = evaluations\n",
        "    return metadata\n",
        "\n",
        "def check_model_metadata_table(readme):\n",
        "    metadata = { \"domain\": {} }\n",
        "    header_re = re.compile(MODEL_HEADER_PATTERN, re.M | re.I)\n",
        "    result = header_re.search(readme)\n",
        "    if (result and len(result.groups()) > 0):\n",
        "        raw_table = result.group(1).strip()\n",
        "        rows = raw_table.split(\"\\n\")\n",
        "        headers = rows[0].split(\"|\")\n",
        "        values = rows[2].split(\"|\")\n",
        "        for i in range(len(headers)):\n",
        "            h = headers[i].strip()\n",
        "            v = values[i].strip()\n",
        "\n",
        "            if h == \"Domain\":\n",
        "                metadata[\"domain\"][\"domain_type\"] = v\n",
        "            elif h == \"Application\":\n",
        "                metadata[\"domain\"][\"tasks\"] = [v]\n",
        "            elif h == \"Industry\":\n",
        "                metadata[\"domain\"][\"industries\"] = [v]\n",
        "            elif h == \"Training Data\" or h == \"Datasets\":\n",
        "                # check if it's a link\n",
        "                link_re = re.compile(LINK_PATTERN)\n",
        "                matches = link_re.findall(v)\n",
        "                datasets = []\n",
        "                if (matches and len(matches) > 0):\n",
        "                    for m in matches:\n",
        "                        datasets.append({\n",
        "                            \"name\": m[0],\n",
        "                            \"url\": m[1]\n",
        "                        })\n",
        "                else:\n",
        "                    datasets.append({\n",
        "                        \"name\": v\n",
        "                    })\n",
        "\n",
        "                metadata[\"training\"] = {\n",
        "                    \"datasets\": datasets\n",
        "                }\n",
        "            elif h == \"Input Data Format\":\n",
        "                if \"definition\" not in metadata:\n",
        "                    metadata[\"definition\"] = {}\n",
        "                if \"input_data_schema\" not in metadata:\n",
        "                    metadata[\"definition\"][\"input_data_schema\"] = {}\n",
        "                metadata[\"definition\"][\"input_data_schema\"][\"description\"] = v\n",
        "            elif h == \"Framework\":\n",
        "                if \"definition\" not in metadata:\n",
        "                    metadata[\"definition\"] = {}\n",
        "                metadata[\"definition\"][\"framework\"] = v\n",
        "\n",
        "    if len(metadata[\"domain\"].keys()) == 0:\n",
        "        del metadata[\"domain\"]\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def readme_parse_text(readme, blob_link, check_datasets=False):\n",
        "    refs = detect_references(readme)\n",
        "    metadata = check_model_metadata_table(readme)\n",
        "    #print(refs)\n",
        "    result = merge_metadata(refs, metadata)\n",
        "\n",
        "    metadata = check_model_value_table(readme)\n",
        "    result = merge_metadata(result, metadata)\n",
        "\n",
        "    author_header_dict = {\n",
        "        \"name\": \"name\",\n",
        "        \"email\": \"email\",\n",
        "        \"Github Profile\": \"github_id\",\n",
        "        \"organization\": \"organization\"\n",
        "    }\n",
        "    result = check_table(readme, \"Contributors\", \"authors\", result, author_header_dict)\n",
        "\n",
        "    # check for special codeblocks\n",
        "    for header, path in CODE_BLOCK_HEADERS_DICT.items():\n",
        "        result = check_codeblock(readme, header, path, result, to_object=True)\n",
        "\n",
        "    # special case of a codeblock containing published docker image address\n",
        "    docker_result = check_codeblock(readme, \"Published Docker Image:\", \"trained_model/binaries\", {})\n",
        "    if \"trained_model\" in docker_result and \"binaries\" in docker_result[\"trained_model\"]:\n",
        "        docker_address = docker_result[\"trained_model\"][\"binaries\"]\n",
        "        docker_result[\"trained_model\"][\"binaries\"] = [{\n",
        "            \"name\": \"Published Docker Image\",\n",
        "            \"type\": \"docker\",\n",
        "            \"description\": \"URL to a docker repository where docker image is published\",\n",
        "            \"connection\": {\n",
        "                \"name\": \"docker_url\",\n",
        "                \"source\": {\n",
        "                    \"url\": docker_address\n",
        "                }\n",
        "            }\n",
        "        }]\n",
        "        result = merge_metadata(result, docker_result)\n",
        "\n",
        "    title = get_readme_title(readme)\n",
        "    if title:\n",
        "        result[\"name\"] = title\n",
        "\n",
        "    if check_datasets:\n",
        "        datasets = detect_datasets(readme, blob_link)\n",
        "        if len(datasets) > 0:\n",
        "            if \"training\" not in result:\n",
        "                result[\"training\"] = {\n",
        "                    \"datasets\": []\n",
        "                }\n",
        "            if \"datasets\" not in result[\"training\"]:\n",
        "                result[\"training\"][\"datasets\"] = []\n",
        "            for d in datasets:\n",
        "                result[\"training\"][\"datasets\"].append(d)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def readme_parse(repo_object, branch_name=None, tree_path=None, check_datasets=False):\n",
        "    result = {}\n",
        "    blob_link = get_blob_link(repo_object, tree_path=tree_path, branch_name=branch_name)\n",
        "    if not tree_path:\n",
        "        readme = get_readme_contents(repo_object)\n",
        "    else:\n",
        "        readme = get_readme_contents_from_path(repo_object, tree_path)\n",
        "\n",
        "    if not readme:\n",
        "        return result\n",
        "\n",
        "    return readme_parse_text(readme, blob_link, check_datasets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqGHZyFd4bvS"
      },
      "source": [
        "import os\n",
        "# import json\n",
        "\n",
        "try:\n",
        "    import importlib.resources as pkg_resources\n",
        "except ImportError:\n",
        "    # Try backported to PY<37 `importlib_resources`.\n",
        "    import importlib_resources as pkg_resources\n",
        "\n",
        "libraries = {}\n",
        "\n",
        "fwArray = []\n",
        "fwEnv = os.getenv(\"frameworks\", None)\n",
        "if fwEnv is not None:\n",
        "    fwArray = fwEnv.split(\",\")\n",
        "else:\n",
        "    with open(\"frameworks.txt\",\"r\") as fwFile:\n",
        "        fwArray = fwFile.read().splitlines()\n",
        "\n",
        "# print(\"Frameworks array:\", \",\".join(fwArray))\n",
        "for f in fwArray:\n",
        "    if \":\" in f:\n",
        "        temp = f.split(\":\")\n",
        "        libraries[temp[0].strip().lower()] = temp[1].strip()\n",
        "    else:\n",
        "        libraries[f.strip().lower()] = f.strip()\n",
        "\n",
        "# print(\"Frameworks libraries:\", json.dumps(libraries, indent=2))\n",
        "\n",
        "\n",
        "def getFrameworks(modules):\n",
        "    fws = []\n",
        "    for mod in modules:\n",
        "        if mod.lower() in libraries.keys():\n",
        "            fws.append(libraries[mod.lower()])\n",
        "    return fws\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGTwf_q17pbs"
      },
      "source": [
        "def extract_framework(repo_url):\n",
        "\n",
        "    url_parts = repo_url.split(\"/\")\n",
        "    for i in range(0, len(url_parts)):\n",
        "        url_parts[i] = url_parts[i].lower()\n",
        "\n",
        "    #print(url_parts)\n",
        "    org = url_parts[3]\n",
        "    reponame = url_parts[4]\n",
        "    repopath = \"/\".join(url_parts[7:len(url_parts)])\n",
        "    if not repopath:\n",
        "        repopath = \"\"\n",
        "\n",
        "    response_json = {'success': False}\n",
        "\n",
        "    response_json['success'] = True\n",
        "    #response_json[\"modules\"] = get_py_modules(org, reponame)\n",
        "    modules = get_py_modules(org, reponame)\n",
        "    response_json[\"frameworks\"] = getFrameworks(modules)\n",
        "    #print(response_json)\n",
        "    return response_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ydrg_lbNQhY",
        "outputId": "78c83ecf-e65d-48a8-8b79-e1ab1a8d896e"
      },
      "source": [
        "from joblib import load\n",
        "import numpy as np\n",
        "\n",
        "# from . import models\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "try:\n",
        "    import importlib.resources as pkg_resources\n",
        "except ImportError:\n",
        "    # Try backported to PY<37 `importlib_resources`.\n",
        "    import importlib_resources as pkg_resources\n",
        "\n",
        "vision_domain_pipeline = load(\"vision-domain-pipeline.joblib\")\n",
        "\n",
        "nlp_domain_pipeline = load(\"nlp-domain-pipeline.joblib\")\n",
        "\n",
        "other_domain_pipeline = load(\"other-domain-pipeline.joblib\")\n",
        "vision_task_pipeline = load(\"vision-task-pipeline.joblib\")\n",
        "\n",
        "vision_le = load(\"vision-le-preprocessing.joblib\")\n",
        "\n",
        "nlp_task_pipeline = load(\"nlp-task-pipeline.joblib\")\n",
        "\n",
        "nlp_le = load(\"nlp-le-preprocessing.joblib\")\n",
        "\n",
        "other_pipeline = load(\"other-task-pipeline.joblib\")\n",
        "\n",
        "other_le = load(\"other-le-preprocessing.joblib\")\n",
        "\n",
        "# vision_domain_pipeline = load('models/vision-domain-pipeline.joblib')\n",
        "# nlp_domain_pipeline = load('models/nlp-domain-pipeline.joblib')\n",
        "# other_domain_pipeline = load('models/other-domain-pipeline.joblib')\n",
        "# vision_task_pipeline = load('models/vision-task-pipeline.joblib')\n",
        "# vision_le = load('models/vision-le-preprocessing.joblib')\n",
        "# nlp_task_pipeline = load('models/nlp-task-pipeline.joblib')\n",
        "# nlp_le = load('models/nlp-le-preprocessing.joblib')\n",
        "# other_pipeline = load('models/other-task-pipeline.joblib')\n",
        "# other_le = load('models/other-le-preprocessing.joblib')\n",
        "\n",
        "\n",
        "def domain_inference(readme):\n",
        "    return_json = { \"domain_type\": \"Unknown\" }\n",
        "\n",
        "    predicted_vision = vision_domain_pipeline.predict_proba([readme])\n",
        "    predicted_nlp = nlp_domain_pipeline.predict_proba([readme])\n",
        "    predicted_other = other_domain_pipeline.predict_proba([readme])\n",
        "\n",
        "    categories = [\"Computer Vision\", \"Natural Language Processing\", \"Other\", \"Unknown\"]\n",
        "    # print(predicted_vision, predicted_nlp, predicted_other)\n",
        "    probs = np.array([ max(predicted_vision[0]), max(predicted_nlp[0]), max(predicted_other[0])])\n",
        "    results = np.array([ np.argmax(predicted_vision[0]), np.argmax(predicted_nlp[0]), np.argmax(predicted_other[0])])\n",
        "    # print(probs, results)\n",
        "\n",
        "    # if only one is True\n",
        "    cat_index = -1\n",
        "    trues = np.sum(results)\n",
        "    if trues == 0:\n",
        "        cat_index = 3\n",
        "    elif trues == 1:\n",
        "        cat_index = np.where(results == 1)[0][0]\n",
        "    elif trues >= 2:\n",
        "        true_index = np.where(results == 1)[0]\n",
        "        for i in range(len(results)):\n",
        "            if i not in true_index:\n",
        "                probs[i] = 0\n",
        "        cat_index = np.argmax(probs)\n",
        "\n",
        "    if cat_index != 3 and probs[cat_index] > 0.5:\n",
        "        return_json[\"domain_type\"] = categories[cat_index]\n",
        "        return_json[\"domain_prob\"] = probs[cat_index]\n",
        "        # do the task or other\n",
        "        if categories[cat_index] == \"Computer Vision\":\n",
        "            predicted = vision_task_pipeline.predict_proba([readme])\n",
        "            # print(\"predicted\", predicted)\n",
        "            prob = max(predicted[0])\n",
        "            result_index = np.argmax(predicted[0])\n",
        "            label = vision_le.inverse_transform([result_index])\n",
        "            return_json[\"task\"] = label[0]\n",
        "            return_json[\"task_prob\"] = prob\n",
        "        elif categories[cat_index] == \"Natural Language Processing\":\n",
        "            predicted = nlp_task_pipeline.predict_proba([readme])\n",
        "            prob = max(predicted[0])\n",
        "            result_index = np.argmax(predicted[0])\n",
        "            label = nlp_le.inverse_transform([result_index])\n",
        "            return_json[\"task\"] = label[0]\n",
        "            return_json[\"task_prob\"] = prob\n",
        "        else:\n",
        "            predicted = other_pipeline.predict_proba([readme])\n",
        "            prob = max(predicted[0])\n",
        "            result_index = np.argmax(predicted[0])\n",
        "            label = other_le.inverse_transform([result_index])\n",
        "            return_json[\"domain_type\"] = label[0]\n",
        "            return_json[\"domain_prob\"] = prob\n",
        "\n",
        "    return return_json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcRouDf5QZYy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4UG8N7XtYY"
      },
      "source": [
        "class AIMMX:\n",
        "    \"\"\" Client for AIMMX.\"\"\"\n",
        "\n",
        "    def __init__(self, public_gh_token, enterprise_gh_creds=None):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        self._public_token = public_gh_token\n",
        "        self._gh = login(token=public_gh_token)\n",
        "        if enterprise_gh_creds:\n",
        "            self._enterprise_login = enterprise_gh_creds[0]\n",
        "            self._enterprise_token = enterprise_gh_creds[1]\n",
        "            gh = GitHubEnterprise(\"https://github.com\")\n",
        "            # self._gh_ent = gh.login(enterprise_gh_creds[0], password=enterprise_gh_creds[1])\n",
        "\n",
        "    def repo_parse(self, repo_url):\n",
        "\n",
        "        result = {}\n",
        "\n",
        "        if \"github.ibm.com\" in repo_url:\n",
        "            if not self._gh_ent:\n",
        "                raise Exception(\"Must provide enterprise GitHub credentials to extract from enterprise repositories.\")\n",
        "            result[\"visibility\"] = {\n",
        "                \"visibility\": \"private\"\n",
        "            }\n",
        "            gh = self._gh_ent\n",
        "        else:\n",
        "            result[\"visibility\"] = {\n",
        "                \"visibility\": \"public\"\n",
        "            }\n",
        "            gh = self._gh\n",
        "\n",
        "        s = repo_url.split(\"/\")\n",
        "        owner = s[3]\n",
        "        repo_name = s[4]\n",
        "        repo = gh.repository(owner, repo_name)\n",
        "\n",
        "        print(\"Extracting AI model metadata from: \", repo)\n",
        "\n",
        "        # check if the url given is a directory or single file\n",
        "        branch = None\n",
        "        tree_path = None\n",
        "        blob_path = None\n",
        "        if len(s) >= 8:\n",
        "            branch = s[6]\n",
        "            if s[5] == \"tree\":\n",
        "                tree_path = s[7:]\n",
        "            if s[5] == \"blob\":\n",
        "                blob_path = s[7:]\n",
        "\n",
        "        # NOTE: need preview accept header for topics, may be brittle\n",
        "        gh.session.headers[\"Accept\"] = \"application/vnd.github.mercy-preview+json\"\n",
        "\n",
        "        result[\"name\"] = repo.name\n",
        "        result[\"definition\"] = {\n",
        "            \"code\": [{\n",
        "                \"type\": \"repo\",\n",
        "                \"repo_type\": \"github\",\n",
        "                \"owner\": owner,\n",
        "                \"owner_type\": repo.owner.as_dict()[\"type\"],\n",
        "                \"name\": repo_name,\n",
        "                \"url\": repo_url,\n",
        "                \"stars\": repo.stargazers_count,\n",
        "                \"pushed_at\": str(repo.pushed_at),\n",
        "                \"created_at\": str(repo.created_at),\n",
        "                \"language\": repo.language\n",
        "            }]\n",
        "        }\n",
        "\n",
        "\n",
        "        # Get all the files in the repo\n",
        "        if not blob_path:\n",
        "            files = get_all_files_from_folder(repo, tree_path=tree_path)\n",
        "        else:\n",
        "            # Single file case\n",
        "            f = (blob_path[-1], \"file\", repo.file_contents(\"/\".join(blob_path)))\n",
        "            files = [f]\n",
        "            tree_path = blob_path[:-1]\n",
        "        blob_link = get_blob_link(repo, tree_path=tree_path, branch_name=branch)\n",
        "\n",
        "        for f in files:\n",
        "            if is_special_file(f[0]):\n",
        "                continue\n",
        "            if is_readme_file(f[0]):\n",
        "                continue\n",
        "            code_meta = {\n",
        "                \"type\": \"code\",\n",
        "                \"name\": f[0],\n",
        "                \"sha\": f[2].sha,\n",
        "                \"filetype\": f[1],\n",
        "                \"connection\": {\n",
        "                    \"name\": \"github_url\",\n",
        "                    \"source\": {}\n",
        "                }\n",
        "            }\n",
        "            if f[1] == \"file\":\n",
        "                code_meta[\"size\"] = f[2].size\n",
        "                code_meta[\"connection\"][\"source\"] = {\"url\": blob_link + \"/\" + f[0]}\n",
        "            elif f[1] == \"dir\":\n",
        "                code_meta[\"num_files\"] = len(f[2].tree)\n",
        "                code_meta[\"connection\"][\"source\"] = {\n",
        "                    \"url\": get_tree_link(repo, tree_path=tree_path, branch_name=branch) + \"/\" + f[0]\n",
        "                }\n",
        "\n",
        "            if is_binary_ext(f[0]):\n",
        "                if \"trained_model\" not in result:\n",
        "                    result[\"trained_model\"] = {}\n",
        "                    if \"binaries\" not in result[\"trained_model\"]:\n",
        "                        result[\"trained_model\"][\"binaries\"] = []\n",
        "\n",
        "                code_meta[\"type\"] = \"binary\"\n",
        "                result[\"trained_model\"][\"binaries\"].append(code_meta)\n",
        "            else:\n",
        "                result[\"definition\"][\"code\"].append(code_meta)\n",
        "\n",
        "            # Caffe2 special case\n",
        "            # NOTE: should later be refactored into a caffe2 specific portion\n",
        "            if f[0] == \"value_info.json\":\n",
        "                schema_contents = f[2].decoded\n",
        "                input_schema = value_json_to_schema(schema_contents)\n",
        "                if input_schema:\n",
        "                    result[\"trained_model\"][\"input_data_schema\"] = input_schema\n",
        "\n",
        "        for c in repo.contributors():\n",
        "            if \"authors\" not in result:\n",
        "                result[\"authors\"] = []\n",
        "            user = gh.user(c.login)\n",
        "            author = {}\n",
        "            if (user.name and len(user.name) > 0):\n",
        "                author[\"name\"] = user.name\n",
        "            else:\n",
        "                author[\"name\"] = c.login\n",
        "            if (user.email and len(user.email) > 0):\n",
        "                author[\"email\"] = user.email\n",
        "            author[\"github_id\"] = c.login\n",
        "            result[\"authors\"].append(author)\n",
        "\n",
        "        # Gets all the topics/tags in GitHub repo\n",
        "        topics = repo.topics()\n",
        "        if topics:\n",
        "            result[\"tags\"] = []\n",
        "            for t in topics.names:\n",
        "                result[\"tags\"].append(t)\n",
        "\n",
        "        repo_desc = None\n",
        "        if repo.description:\n",
        "            result[\"description\"] = repo.description\n",
        "            result[\"definition\"][\"code\"][0][\"description\"] = repo.description\n",
        "            repo_desc = repo.description\n",
        "\n",
        "        extraction = {\n",
        "            \"type\": \"github\",\n",
        "            \"url\": repo_url,\n",
        "            \"stars\": repo.stargazers_count,\n",
        "            \"issues\": repo.has_issues\n",
        "        }\n",
        "\n",
        "        # Gets readme and does analysis\n",
        "        readme_content = None\n",
        "        # For single file case, treats docstring as the readme\n",
        "        if blob_path:\n",
        "            readme_content = get_readme_contents_from_docstring(repo, files[0][2].sha)\n",
        "            if readme_content is not None:\n",
        "                extraction[\"readme\"] = readme_content\n",
        "                extraction[\"readme_url\"] = repo_url\n",
        "        # Subfolder case and also if single file does not contain docstring\n",
        "        if tree_path and (not blob_path or readme_content is None):\n",
        "            readme_content = get_readme_contents_from_path(repo, tree_path)\n",
        "            if readme_content is not None:\n",
        "                extraction[\"readme\"] = readme_content\n",
        "                extraction[\"readme_url\"] = repo_url\n",
        "        # if subfolder case does not find a README, try with repo-level README\n",
        "        if readme_content is None:\n",
        "            readme_content = get_readme_contents(repo)\n",
        "            if readme_content is not None:\n",
        "                extraction[\"readme\"] = readme_content\n",
        "                extraction[\"readme_url\"] = \"/\".join(s[:5])\n",
        "\n",
        "        result[\"extraction\"] = [extraction]\n",
        "\n",
        "        repo_license = None\n",
        "        try:\n",
        "            if repo.license():\n",
        "                repo_license = repo.license().license.name\n",
        "                result[\"visibility\"][\"license\"] = repo_license\n",
        "                result[\"definition\"][\"code\"][0][\"license\"] = repo_license\n",
        "        except exceptions.NotFoundError:\n",
        "            pass\n",
        "\n",
        "        # outdated framework extraction\n",
        "        # frameworks = repo_framework(repo)\n",
        "        # if (frameworks and len(frameworks.keys()) > 0):\n",
        "        #     result[\"definition\"][\"code\"][0][\"framework\"] = []\n",
        "        #     for fw,version in frameworks.items():\n",
        "        #         fw_insert = {\n",
        "        #             \"name\": fw\n",
        "        #         }\n",
        "        #         if version != \"\":\n",
        "        #             fw_insert[\"version\"] = version\n",
        "        #         # NOTE: for now, framework is singular, change this later\n",
        "        #         result[\"definition\"][\"code\"][0][\"framework\"] = fw_insert\n",
        "\n",
        "        readme_info = readme_parse(repo, branch_name=branch, check_datasets=True)\n",
        "        readme_name = None\n",
        "        if \"name\" in readme_info:\n",
        "            readme_name = readme_info[\"name\"]\n",
        "        result = merge_metadata(result, readme_info)\n",
        "\n",
        "        if tree_path:\n",
        "            readme_info = readme_parse(repo, branch_name=branch, tree_path=tree_path, check_datasets=True)\n",
        "            # special case for subfolders, if there's no other name, take it from the folder\n",
        "            if \"name\" not in readme_info:\n",
        "                readme_info[\"name\"] = \"/\".join(tree_path)\n",
        "            result = merge_metadata(result, readme_info)\n",
        "\n",
        "        if blob_path:\n",
        "            readme_info = readme_parse_text(readme_content, blob_link, check_datasets=True)\n",
        "            result = merge_metadata(result, readme_info)\n",
        "\n",
        "        # special case to add frameworks to code files\n",
        "        if \"definition\" in result and \"framework\" in result[\"definition\"]:\n",
        "            if \"code\" in result[\"definition\"]:\n",
        "                result[\"definition\"][\"code\"][0][\"framework\"] = {\n",
        "                    \"name\": result[\"definition\"].pop(\"framework\")\n",
        "                }\n",
        "\n",
        "        # Special files check, doesn't make sense in single-file case\n",
        "        if not blob_path:\n",
        "            specialfiles = detect_special_files(repo, tree_path=tree_path)\n",
        "            result = merge_metadata(result, specialfiles)\n",
        "\n",
        "        # if README exists, attempt domain Inference and dataset detection\n",
        "        if readme_content:\n",
        "            plain_readme = readme_cleanup(readme_content)\n",
        "\n",
        "            abstracts = \"\"\n",
        "            if \"references\" in result:\n",
        "                for r in result[\"references\"]:\n",
        "                    if \"abstract\" in r:\n",
        "                        abstract = r[\"abstract\"]\n",
        "                        abstract = abstract.strip().replace(\"\\n\", \" \")\n",
        "                        abstracts +=  \"\\n{}\".format(abstract)\n",
        "            if len(abstracts) > 0:\n",
        "                plain_readme += abstracts\n",
        "\n",
        "            domain = domain_inference(plain_readme)\n",
        "            result[\"domain\"] = domain\n",
        "\n",
        "            # NOTE: currently only running on abstracts, should refactor to run on abstract + readme\n",
        "            abs_datasets = detect_datasets_list(abstracts)\n",
        "            if len(abs_datasets) > 0:\n",
        "\n",
        "                if \"training\" in result and \"datasets\" in result[\"training\"]:\n",
        "                    dataset_names = set()\n",
        "                    datasets_to_add = []\n",
        "                    for d in result[\"training\"][\"datasets\"]:\n",
        "                        dataset_names.add(d[\"name\"].lower())\n",
        "                    for d in abs_datasets:\n",
        "                        if d[\"name\"].lower() not in dataset_names:\n",
        "                            datasets_to_add.append(d)\n",
        "                    result[\"training\"][\"datasets\"] += datasets_to_add\n",
        "                else:\n",
        "                    result[\"training\"] = { \"datasets\": abs_datasets }\n",
        "\n",
        "        # attempt to extract framework via cloning\n",
        "        framework_result = extract_framework(repo_url)\n",
        "        if framework_result[\"success\"]:\n",
        "            if \"frameworks\" in framework_result:\n",
        "                result[\"extraction\"][0][\"frameworks\"] = framework_result[\"frameworks\"]\n",
        "                if \"code\" in result[\"definition\"]:\n",
        "                    result[\"definition\"][\"code\"][0][\"frameworks\"] = framework_result[\"frameworks\"]\n",
        "            if \"modules\" in framework_result:\n",
        "                result[\"extraction\"][0][\"modules\"] = framework_result[\"modules\"]\n",
        "                if \"code\" in result[\"definition\"]:\n",
        "                    result[\"definition\"][\"code\"][0][\"modules\"] = framework_result[\"modules\"]\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7CnCcE_H5TY"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2MpG4i-5ajz"
      },
      "source": [
        "# To generate a github api key\n",
        "\n",
        "#On GitHub, click your profile picture in the top-right\n",
        "\n",
        "#Settings > Developer Settings > Personal access tokens > \"Generate new token\"\n",
        "\n",
        "\n",
        "a_aimmx = AIMMX(public_gh_token=\"8166550b79e39b73676cbe1f255b705e36fb5f0e\", enterprise_gh_creds=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNKrg7Ux6S5m",
        "outputId": "66b3bbc4-2239-40a7-e72e-a4d927f8e112"
      },
      "source": [
        "#example github links\n",
        "\n",
        "# https://github.com/strawlab/braincode\n",
        "# https://github.com/tensorlayer/srgan\n",
        "# https://github.com/TachibanaYoshino/AnimeGAN\n",
        "# https://github.com/YunjaeChoi/vaemols\n",
        "# https://github.com/CR-Gjx/LeakGAN\n",
        "\n",
        "import pprint\n",
        "import git\n",
        "metadata = a_aimmx.repo_parse(repo_url=\"https://github.com/kweonwooj/papers\")\n",
        "#metadata can also be downloaded as json file\n",
        "# metadata = aimmx.repo_parse(repo)\n",
        "#     with open(metadata[\"name\"] + \".json\", \"w\") as f:\n",
        "#         json.dump(metadata, f, indent=4)\n",
        "pprint.pprint(metadata, indent=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting AI model metadata from:  kweonwooj/papers\n",
            "{   'authors': [   {   'email': 'kweonwooj@gmail.com',\n",
            "                       'github_id': 'kweonwooj',\n",
            "                       'name': 'Kweon Woo Jung(Chris Jung)'}],\n",
            "    'definition': {   'code': [   {   'created_at': '2017-09-27 02:30:43+00:00',\n",
            "                                      'description': 'summary of ML papers '\n",
            "                                                     \"I've read\",\n",
            "                                      'frameworks': [],\n",
            "                                      'language': None,\n",
            "                                      'name': 'papers',\n",
            "                                      'owner': 'kweonwooj',\n",
            "                                      'owner_type': 'User',\n",
            "                                      'pushed_at': '2018-07-27 07:30:07+00:00',\n",
            "                                      'repo_type': 'github',\n",
            "                                      'stars': 281,\n",
            "                                      'type': 'repo',\n",
            "                                      'url': 'https://github.com/kweonwooj/papers'}]},\n",
            "    'description': \"summary of ML papers I've read\",\n",
            "    'domain': {'domain_type': 'Unknown'},\n",
            "    'extraction': [   {   'frameworks': [],\n",
            "                          'issues': True,\n",
            "                          'readme': \"- Summary of ML papers I've read in \"\n",
            "                                    'recorded in Issues\\n'\n",
            "                                    '- Questions to ask myself when reading '\n",
            "                                    'papers.. (Reference : [Guide to Reading '\n",
            "                                    'Academic Research '\n",
            "                                    'Papers](https://towardsdatascience.com/guide-to-reading-academic-research-papers-c69c21619de6))\\n'\n",
            "                                    '```\\n'\n",
            "                                    '1. What previous research and ideas were '\n",
            "                                    'cited that this paper is building off of? '\n",
            "                                    '(this info tends to live in the '\n",
            "                                    'introduction)\\n'\n",
            "                                    '2. Was there reasoning for performing '\n",
            "                                    'this research, if so what was it? '\n",
            "                                    '(introduction section)\\n'\n",
            "                                    '3. Clearly list out the objectives of the '\n",
            "                                    'study\\n'\n",
            "                                    '4. Was any equipment/software used? '\n",
            "                                    '(methods section)\\n'\n",
            "                                    '5. What variables were measured during '\n",
            "                                    'experimentation? (methods)\\n'\n",
            "                                    '6. Were any statistical tests used? What '\n",
            "                                    'were their results? (methods/results '\n",
            "                                    'section)\\n'\n",
            "                                    '7. What are the main findings? (results '\n",
            "                                    'section)\\n'\n",
            "                                    '8. How do these results fit into the '\n",
            "                                    'context of other research and their '\n",
            "                                    \"'field'? (discussion section)\\n\"\n",
            "                                    '9. Explain each figure and discuss their '\n",
            "                                    'significance.\\n'\n",
            "                                    '10. Can the results be reproduced and is '\n",
            "                                    'there any code available?\\n'\n",
            "                                    '11. Name the authors, year, and title of '\n",
            "                                    'the paper!\\n'\n",
            "                                    '12. Are any of the authors familiar, do '\n",
            "                                    'you know their previous work? \\n'\n",
            "                                    '13. What key terms and concepts do I not '\n",
            "                                    'know and need to look up in a dictionary, '\n",
            "                                    'textbook, or ask someone?\\n'\n",
            "                                    '14. What are your thoughts on the '\n",
            "                                    'results? Do they seem valid?\\n'\n",
            "                                    '```\\n',\n",
            "                          'readme_url': 'https://github.com/kweonwooj/papers',\n",
            "                          'stars': 281,\n",
            "                          'type': 'github',\n",
            "                          'url': 'https://github.com/kweonwooj/papers'}],\n",
            "    'name': '```',\n",
            "    'tags': [],\n",
            "    'visibility': {'visibility': 'public'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II8Mhitwo00n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}